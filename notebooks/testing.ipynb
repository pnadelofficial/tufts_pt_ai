{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from chatting import Stateflow\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI client config of GPTAssistantAgent(QUADL_CAPTE) - model: gpt-4\n",
      "No instructions were provided for given assistant. Using existing instructions from assistant API.\n",
      "No tools were provided for given assistant. Using existing tools from assistant API.\n",
      "OpenAI client config of GPTAssistantAgent(s_critic) - model: gpt-4\n",
      "No instructions were provided for given assistant. Using existing instructions from assistant API.\n",
      "No tools were provided for given assistant. Using existing tools from assistant API.\n",
      "OpenAI client config of GPTAssistantAgent(s_critic2) - model: gpt-4\n",
      "No instructions were provided for given assistant. Using existing instructions from assistant API.\n",
      "No tools were provided for given assistant. Using existing tools from assistant API.\n"
     ]
    }
   ],
   "source": [
    "sf = Stateflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': 'Create 5 multiple-choice items and output them in the appropriate format, including labels: Learning Objective(LO), CAPTE Statndard(CAPTE), Answer(A), Sentence quote from content(S), Bloom(B)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openning = \"Create 5 multiple-choice items and output them in the appropriate format, including labels: Learning Objective(LO), CAPTE Statndard(CAPTE), Answer(A), Sentence quote from content(S), Bloom(B)\"\n",
    "sf.initializer.generate_reply(messages=[{\"content\":openning, \"role\":\"user\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Create 5 multiple-choice items and output them in the appropriate format, including labels: Learning Objective(LO), CAPTE Statndard(CAPTE), Answer(A), Sentence quote from content(S), Bloom(B)', 'role': 'assistant'}], summary='Create 5 multiple-choice items and output them in the appropriate format, including labels: Learning Objective(LO), CAPTE Statndard(CAPTE), Answer(A), Sentence quote from content(S), Bloom(B)', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           UserProxyAgent\n",
      "\u001b[0;31mString form:\u001b[0m    <autogen.agentchat.user_proxy_agent.UserProxyAgent object at 0x12eac7010>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pt_openai/lib/python3.11/site-packages/autogen/agentchat/user_proxy_agent.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mUserProxyAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConversableAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"(In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    UserProxyAgent is a subclass of ConversableAgent configured with `human_input_mode` to ALWAYS\u001b[0m\n",
      "\u001b[0;34m    and `llm_config` to False. By default, the agent will prompt for human input every time a message is received.\u001b[0m\n",
      "\u001b[0;34m    Code execution is enabled by default. LLM-based auto reply is disabled by default.\u001b[0m\n",
      "\u001b[0;34m    To modify auto reply, register a method with [`register_reply`](conversable_agent#register_reply).\u001b[0m\n",
      "\u001b[0;34m    To modify the way to get human input, override `get_human_input` method.\u001b[0m\n",
      "\u001b[0;34m    To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,\u001b[0m\n",
      "\u001b[0;34m    `run_code`, and `execute_function` methods respectively.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Default UserProxyAgent.description values, based on human_input_mode\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mDEFAULT_USER_PROXY_AGENT_DESCRIPTIONS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"ALWAYS\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"An attentive HUMAN user who can answer questions about the task, and can perform tasks such as running Python code or inputting command line commands at a Linux terminal and reporting back the execution results.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"TERMINATE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"NEVER\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mis_termination_msg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_consecutive_auto_reply\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mhuman_input_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ALWAYS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TERMINATE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NEVER\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ALWAYS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfunction_map\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcode_execution_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdefault_auto_reply\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mllm_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msystem_message\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            name (str): name of the agent.\u001b[0m\n",
      "\u001b[0;34m            is_termination_msg (function): a function that takes a message in the form of a dictionary\u001b[0m\n",
      "\u001b[0;34m                and returns a boolean value indicating if this received message is a termination message.\u001b[0m\n",
      "\u001b[0;34m                The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\u001b[0m\n",
      "\u001b[0;34m            max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\u001b[0m\n",
      "\u001b[0;34m                default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\u001b[0m\n",
      "\u001b[0;34m                The limit only plays a role when human_input_mode is not \"ALWAYS\".\u001b[0m\n",
      "\u001b[0;34m            human_input_mode (str): whether to ask for human inputs every time a message is received.\u001b[0m\n",
      "\u001b[0;34m                Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\u001b[0m\n",
      "\u001b[0;34m                (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\u001b[0m\n",
      "\u001b[0;34m                    Under this mode, the conversation stops when the human input is \"exit\",\u001b[0m\n",
      "\u001b[0;34m                    or when is_termination_msg is True and there is no human input.\u001b[0m\n",
      "\u001b[0;34m                (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\u001b[0m\n",
      "\u001b[0;34m                    the number of auto reply reaches the max_consecutive_auto_reply.\u001b[0m\n",
      "\u001b[0;34m                (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\u001b[0m\n",
      "\u001b[0;34m                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\u001b[0m\n",
      "\u001b[0;34m            function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions.\u001b[0m\n",
      "\u001b[0;34m            code_execution_config (dict or False): config for the code execution.\u001b[0m\n",
      "\u001b[0;34m                To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\u001b[0m\n",
      "\u001b[0;34m                - work_dir (Optional, str): The working directory for the code execution.\u001b[0m\n",
      "\u001b[0;34m                    If None, a default working directory will be used.\u001b[0m\n",
      "\u001b[0;34m                    The default working directory is the \"extensions\" directory under\u001b[0m\n",
      "\u001b[0;34m                    \"path_to_autogen\".\u001b[0m\n",
      "\u001b[0;34m                - use_docker (Optional, list, str or bool): The docker image to use for code execution.\u001b[0m\n",
      "\u001b[0;34m                    Default is True, which means the code will be executed in a docker container. A default list of images will be used.\u001b[0m\n",
      "\u001b[0;34m                    If a list or a str of image name(s) is provided, the code will be executed in a docker container\u001b[0m\n",
      "\u001b[0;34m                    with the first image successfully pulled.\u001b[0m\n",
      "\u001b[0;34m                    If False, the code will be executed in the current environment.\u001b[0m\n",
      "\u001b[0;34m                    We strongly recommend using docker for code execution.\u001b[0m\n",
      "\u001b[0;34m                - timeout (Optional, int): The maximum execution time in seconds.\u001b[0m\n",
      "\u001b[0;34m                - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\u001b[0m\n",
      "\u001b[0;34m            default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.\u001b[0m\n",
      "\u001b[0;34m            llm_config (dict or False or None): llm inference configuration.\u001b[0m\n",
      "\u001b[0;34m                Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create)\u001b[0m\n",
      "\u001b[0;34m                for available options.\u001b[0m\n",
      "\u001b[0;34m                Default to False, which disables llm-based auto reply.\u001b[0m\n",
      "\u001b[0;34m                When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\u001b[0m\n",
      "\u001b[0;34m            system_message (str or List): system message for ChatCompletion inference.\u001b[0m\n",
      "\u001b[0;34m                Only used when llm_config is not False. Use it to reprogram the agent.\u001b[0m\n",
      "\u001b[0;34m            description (str): a short description of the agent. This description is used by other agents\u001b[0m\n",
      "\u001b[0;34m                (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msystem_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mis_termination_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_termination_msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmax_consecutive_auto_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_consecutive_auto_reply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mhuman_input_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman_input_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mfunction_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcode_execution_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_execution_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdefault_auto_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_auto_reply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdescription\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_USER_PROXY_AGENT_DESCRIPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhuman_input_mode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlog_new_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Args:\n",
      "    name (str): name of the agent.\n",
      "    is_termination_msg (function): a function that takes a message in the form of a dictionary\n",
      "        and returns a boolean value indicating if this received message is a termination message.\n",
      "        The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "    max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\n",
      "        default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "        The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "    human_input_mode (str): whether to ask for human inputs every time a message is received.\n",
      "        Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "        (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "            Under this mode, the conversation stops when the human input is \"exit\",\n",
      "            or when is_termination_msg is True and there is no human input.\n",
      "        (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "            the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "        (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "            when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "    function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions.\n",
      "    code_execution_config (dict or False): config for the code execution.\n",
      "        To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
      "        - work_dir (Optional, str): The working directory for the code execution.\n",
      "            If None, a default working directory will be used.\n",
      "            The default working directory is the \"extensions\" directory under\n",
      "            \"path_to_autogen\".\n",
      "        - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
      "            Default is True, which means the code will be executed in a docker container. A default list of images will be used.\n",
      "            If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "            with the first image successfully pulled.\n",
      "            If False, the code will be executed in the current environment.\n",
      "            We strongly recommend using docker for code execution.\n",
      "        - timeout (Optional, int): The maximum execution time in seconds.\n",
      "        - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\n",
      "    default_auto_reply (str or dict or None): the default auto reply message when no code execution or llm based reply is generated.\n",
      "    llm_config (dict or False or None): llm inference configuration.\n",
      "        Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create)\n",
      "        for available options.\n",
      "        Default to False, which disables llm-based auto reply.\n",
      "        When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\n",
      "    system_message (str or List): system message for ChatCompletion inference.\n",
      "        Only used when llm_config is not False. Use it to reprogram the agent.\n",
      "    description (str): a short description of the agent. This description is used by other agents\n",
      "        (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)"
     ]
    }
   ],
   "source": [
    "sf.initializer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
